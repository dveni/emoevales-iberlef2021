{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "royal-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "from nlp import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "centered-cornell",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Using custom data configuration default\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', delimiter='\\t',\n",
    "                       data_files={'train': 'data/EmoEvalEs/train.tsv',\n",
    "                                    'validation': 'data/EmoEvalEs/dev.tsv',})\n",
    "test = load_dataset('csv', delimiter='\\t', data_files={'test': 'data/EmoEvalEs/emoevales_test.tsv'})\n",
    "test = pd.read_csv('data/EmoEvalEs/emoevales_test.tsv', sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "available-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from scipy.sparse import hstack\n",
    "ohe = OneHotEncoder()\n",
    "feat1_train = ohe.fit_transform(np.array(dataset['train']['event']).reshape(-1,1))\n",
    "feat1_dev = ohe.transform(np.array(dataset['validation']['event']).reshape(-1,1))\n",
    "feat1_test = ohe.transform(np.array(test['event']).reshape(-1,1))\n",
    "\n",
    "oe = OrdinalEncoder()\n",
    "feat2_train = oe.fit_transform(np.array(dataset['train']['offensive']).reshape(-1,1))\n",
    "feat2_dev = oe.transform(np.array(dataset['validation']['offensive']).reshape(-1,1))\n",
    "feat2_test = oe.transform(np.array(test['offensive']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "friendly-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feats(name, fold):\n",
    "    path = 'feats/{}_{}.pck'.format(name, fold)\n",
    "    with open(path, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "meaningcloud_dev = load_feats('meaningcloud', 'dev')\n",
    "meaningcloud_test = load_feats('meaningcloud', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "robust-article",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev ['xlmroberta', 'w2v', 'ngramfeats', 'tfidf', 'simon', 'ngram']\n",
      "test ['xlmroberta', 'w2v', 'ngramfeats', 'tfidf', 'simon', 'ngram']\n"
     ]
    }
   ],
   "source": [
    "def read_preds(name, fold):\n",
    "    path = 'preds_{}/{}.pck'.format(fold, name)\n",
    "    with open(path, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "preds = {'dev': dict(), 'test': dict()}\n",
    "\n",
    "preds['dev']['xlmroberta']= read_preds('xlmroberta', 'dev')\\\n",
    "    .set_index('id').loc[dataset['validation']['id']]['emotion'].values\n",
    "preds['test']['xlmroberta']= pd.read_csv('preds_test/submission-roberta-final.tsv',\n",
    "                                         header=None, sep='\\t', names=['id', 'emotion'])\\\n",
    "    .set_index('id').loc[test['id']]['emotion'].values\n",
    "\n",
    "for fold in ('dev', 'test'):\n",
    "    for preds_dev in glob('preds_{}/*.pck'.format(fold)):\n",
    "        name = os.path.splitext(os.path.basename(preds_dev))[0]\n",
    "        if name in ('xlmroberta', 'xlmroberta-extrafeatures'):\n",
    "            continue\n",
    "        preds[fold][name] = read_preds(name, fold)\n",
    "print('dev', list(preds['dev'].keys()))\n",
    "print('test', list(preds['dev'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "alpha-cincinnati",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlmroberta\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.70      0.68      0.69        85\n",
      "     disgust       0.00      0.00      0.00        16\n",
      "        fear       1.00      0.44      0.62         9\n",
      "         joy       0.70      0.66      0.68       181\n",
      "      others       0.73      0.85      0.78       414\n",
      "     sadness       0.80      0.80      0.80       104\n",
      "    surprise       0.60      0.09      0.15        35\n",
      "\n",
      "    accuracy                           0.73       844\n",
      "   macro avg       0.65      0.50      0.53       844\n",
      "weighted avg       0.71      0.73      0.71       844\n",
      "\n",
      "\n",
      "w2v\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.60      0.44      0.50        85\n",
      "     disgust       0.00      0.00      0.00        16\n",
      "        fear       1.00      0.11      0.20         9\n",
      "         joy       0.54      0.38      0.44       181\n",
      "      others       0.64      0.81      0.72       414\n",
      "     sadness       0.68      0.66      0.67       104\n",
      "    surprise       0.12      0.11      0.12        35\n",
      "\n",
      "    accuracy                           0.61       844\n",
      "   macro avg       0.51      0.36      0.38       844\n",
      "weighted avg       0.59      0.61      0.59       844\n",
      "\n",
      "\n",
      "ngramfeats\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.62      0.44      0.51        85\n",
      "     disgust       0.00      0.00      0.00        16\n",
      "        fear       0.67      0.22      0.33         9\n",
      "         joy       0.58      0.47      0.52       181\n",
      "      others       0.66      0.85      0.75       414\n",
      "     sadness       0.77      0.62      0.68       104\n",
      "    surprise       0.09      0.06      0.07        35\n",
      "\n",
      "    accuracy                           0.64       844\n",
      "   macro avg       0.48      0.38      0.41       844\n",
      "weighted avg       0.62      0.64      0.62       844\n",
      "\n",
      "\n",
      "tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.65      0.28      0.39        85\n",
      "     disgust       0.00      0.00      0.00        16\n",
      "        fear       1.00      0.11      0.20         9\n",
      "         joy       0.60      0.49      0.54       181\n",
      "      others       0.72      0.71      0.72       414\n",
      "     sadness       0.78      0.60      0.67       104\n",
      "    surprise       0.06      0.31      0.11        35\n",
      "\n",
      "    accuracy                           0.57       844\n",
      "   macro avg       0.54      0.36      0.38       844\n",
      "weighted avg       0.66      0.57      0.60       844\n",
      "\n",
      "\n",
      "simon\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.64      0.41      0.50        85\n",
      "     disgust       0.00      0.00      0.00        16\n",
      "        fear       0.00      0.00      0.00         9\n",
      "         joy       0.64      0.46      0.54       181\n",
      "      others       0.66      0.90      0.76       414\n",
      "     sadness       0.74      0.64      0.69       104\n",
      "    surprise       0.00      0.00      0.00        35\n",
      "\n",
      "    accuracy                           0.66       844\n",
      "   macro avg       0.38      0.35      0.36       844\n",
      "weighted avg       0.62      0.66      0.62       844\n",
      "\n",
      "\n",
      "ngram\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.74      0.29      0.42        85\n",
      "     disgust       0.00      0.00      0.00        16\n",
      "        fear       0.67      0.22      0.33         9\n",
      "         joy       0.58      0.45      0.51       181\n",
      "      others       0.64      0.86      0.73       414\n",
      "     sadness       0.81      0.56      0.66       104\n",
      "    surprise       0.15      0.14      0.14        35\n",
      "\n",
      "    accuracy                           0.63       844\n",
      "   macro avg       0.51      0.36      0.40       844\n",
      "weighted avg       0.62      0.63      0.60       844\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "metrics = dict()\n",
    "for name in preds['dev'].keys():\n",
    "    labels = dataset['validation']['emotion']\n",
    "    class_rep = classification_report(labels, preds['dev'][name])\n",
    "    metrics[name] = {\n",
    "        'acc': accuracy_score(labels, preds['dev'][name]),\n",
    "        'f1': f1_score(labels, preds['dev'][name], average='weighted')\n",
    "    }\n",
    "    print(name)\n",
    "    print(class_rep)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "diverse-sleeping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>xlmroberta</th>\n",
       "      <td>0.731043</td>\n",
       "      <td>0.710999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v</th>\n",
       "      <td>0.610190</td>\n",
       "      <td>0.588116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngramfeats</th>\n",
       "      <td>0.642180</td>\n",
       "      <td>0.619510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>0.569905</td>\n",
       "      <td>0.596302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simon</th>\n",
       "      <td>0.663507</td>\n",
       "      <td>0.624579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram</th>\n",
       "      <td>0.626777</td>\n",
       "      <td>0.601933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 acc        f1\n",
       "xlmroberta  0.731043  0.710999\n",
       "w2v         0.610190  0.588116\n",
       "ngramfeats  0.642180  0.619510\n",
       "tfidf       0.569905  0.596302\n",
       "simon       0.663507  0.624579\n",
       "ngram       0.626777  0.601933"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = pd.DataFrame(metrics).T\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-grill",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-memphis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "banned-family",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble_lr+feats+mc\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.68      0.62      0.65        85\n",
      "     disgust       0.19      0.19      0.19        16\n",
      "        fear       0.42      0.56      0.48         9\n",
      "         joy       0.69      0.65      0.67       181\n",
      "      others       0.75      0.82      0.79       414\n",
      "     sadness       0.80      0.80      0.80       104\n",
      "    surprise       0.42      0.14      0.21        35\n",
      "\n",
      "    accuracy                           0.72       844\n",
      "   macro avg       0.56      0.54      0.54       844\n",
      "weighted avg       0.71      0.72      0.71       844\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "X_dev = pd.DataFrame(preds['dev']).values\n",
    "X_dev = ohe.fit_transform(X_dev)\n",
    "y_dev = dataset['validation']['emotion']\n",
    "X_test = ohe.transform(pd.DataFrame(preds['test']).values)\n",
    "\n",
    "\n",
    "final_preds = dict()\n",
    "# lr\n",
    "# lr = LogisticRegression(solver='liblinear', max_iter=200, random_state=42)\n",
    "# distributions = dict(C=uniform(loc=0, scale=10), penalty=['l2', 'l1'])\n",
    "# clf = RandomizedSearchCV(lr, distributions, random_state=42)\n",
    "# clf.fit(X_dev, y_dev)\n",
    "# final_preds['ensemble_lr'] = cross_val_predict(clf.best_estimator_, X_dev, y_dev)\n",
    "\n",
    "# random forest\n",
    "# rf = RandomForestClassifier()\n",
    "# distributions = dict(n_estimators=randint(1, 500))\n",
    "# clf = RandomizedSearchCV(rf, distributions, random_state=42)\n",
    "# clf.fit(X_dev, y_dev)\n",
    "# final_preds['ensemble_rf'] = cross_val_predict(clf.best_estimator_, X_dev, y_dev)\n",
    "\n",
    "#SVM\n",
    "# svm = SVC(random_state=42)\n",
    "# distributions = dict(C=uniform(loc=0, scale=10))\n",
    "# clf = RandomizedSearchCV(svm, distributions, random_state=42)\n",
    "# clf.fit(X_dev, y_dev)\n",
    "# final_preds['ensemble_svm'] = cross_val_predict(clf.best_estimator_, X_dev, y_dev)\n",
    "\n",
    "# lr + feats\n",
    "# X_dev_feats =hstack((X_dev, feat1_dev.todense(), feat2_dev))\n",
    "# lr = LogisticRegression(solver='liblinear', max_iter=300, random_state=42,\n",
    "#                         class_weight='balanced',)\n",
    "# distributions = dict(C=uniform(loc=0, scale=10), penalty=['l2', 'l1'])\n",
    "# clf = RandomizedSearchCV(lr, distributions, random_state=42)\n",
    "# clf.fit(X_dev, y_dev)\n",
    "# final_preds['ensemble_lr+feats'] = cross_val_predict(clf.best_estimator_, X_dev_feats, y_dev)\n",
    "\n",
    "\n",
    "# svm + feats\n",
    "# X_dev_feats =hstack((X_dev, feat1_dev.todense(), feat2_dev))\n",
    "# svm =svm = SVC(random_state=42)\n",
    "# distributions = dict(C=uniform(loc=0, scale=10))\n",
    "# clf = RandomizedSearchCV(svm, distributions, random_state=42)\n",
    "# clf.fit(X_dev, y_dev)\n",
    "# final_preds['ensemble_svm+feats'] = cross_val_predict(clf.best_estimator_, X_dev_feats, y_dev)\n",
    "\n",
    "# random forest\n",
    "# X_dev_feats =hstack((X_dev, feat1_dev.todense(), feat2_dev))\n",
    "# rf = RandomForestClassifier()\n",
    "# distributions = dict(n_estimators=randint(1, 500))\n",
    "# clf = RandomizedSearchCV(rf, distributions, random_state=42)\n",
    "# clf.fit(X_dev_feats, y_dev)\n",
    "# final_preds['ensemble_rf+feats'] = cross_val_predict(clf.best_estimator_, X_dev, y_dev)\n",
    "\n",
    "# lr + feats + meaningcloud\n",
    "X_dev_feats_mc =hstack((X_dev, feat1_dev.todense(), feat2_dev, meaningcloud_dev))\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear', max_iter=300, random_state=42,\n",
    "                        class_weight='balanced',)\n",
    "pipe = Pipeline([\n",
    "    ('pprocess', Normalizer()),\n",
    "    ('lr', lr),\n",
    "])\n",
    "distributions = dict(lr__C=uniform(loc=0, scale=10), lr__penalty=['l2', 'l1'],)\n",
    "clf = RandomizedSearchCV(pipe, distributions, random_state=42)\n",
    "clf.fit(X_dev_feats_mc, y_dev)\n",
    "final_preds['ensemble_lr+feats+mc'] = cross_val_predict(clf.best_estimator_, X_dev_feats_mc, y_dev)\n",
    "\n",
    "for name, p in final_preds.items():\n",
    "    print(name)\n",
    "    print(classification_report(y_dev, final_preds[name]))\n",
    "    print()\n",
    "    metrics.loc[name] = [\n",
    "        accuracy_score(y_dev, final_preds[name]),\n",
    "        f1_score(y_dev, final_preds[name], average='weighted'),\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "detected-notice",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('pprocess', Normalizer()),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=0.5641157902710026,\n",
       "                                    class_weight='balanced', max_iter=300,\n",
       "                                    penalty='l1', random_state=42,\n",
       "                                    solver='liblinear'))])"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "distinct-cleveland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>xlmroberta</th>\n",
       "      <td>0.731043</td>\n",
       "      <td>0.710999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v</th>\n",
       "      <td>0.610190</td>\n",
       "      <td>0.588116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngramfeats</th>\n",
       "      <td>0.642180</td>\n",
       "      <td>0.619510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>0.569905</td>\n",
       "      <td>0.596302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simon</th>\n",
       "      <td>0.663507</td>\n",
       "      <td>0.624579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram</th>\n",
       "      <td>0.626777</td>\n",
       "      <td>0.601933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_lr</th>\n",
       "      <td>0.715640</td>\n",
       "      <td>0.689328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_rf</th>\n",
       "      <td>0.693128</td>\n",
       "      <td>0.675578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_lr+feats</th>\n",
       "      <td>0.709716</td>\n",
       "      <td>0.707557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_rf+feats</th>\n",
       "      <td>0.680095</td>\n",
       "      <td>0.664358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_lr+feats+mc</th>\n",
       "      <td>0.719194</td>\n",
       "      <td>0.710522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           acc        f1\n",
       "xlmroberta            0.731043  0.710999\n",
       "w2v                   0.610190  0.588116\n",
       "ngramfeats            0.642180  0.619510\n",
       "tfidf                 0.569905  0.596302\n",
       "simon                 0.663507  0.624579\n",
       "ngram                 0.626777  0.601933\n",
       "ensemble_lr           0.715640  0.689328\n",
       "ensemble_rf           0.693128  0.675578\n",
       "ensemble_lr+feats     0.709716  0.707557\n",
       "ensemble_rf+feats     0.680095  0.664358\n",
       "ensemble_lr+feats+mc  0.719194  0.710522"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "interracial-snowboard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                      |      acc |       f1 |\n",
      "|:---------------------|---------:|---------:|\n",
      "| xlmroberta           | 0.731043 | 0.710999 |\n",
      "| w2v                  | 0.61019  | 0.588116 |\n",
      "| ngramfeats           | 0.64218  | 0.61951  |\n",
      "| tfidf                | 0.569905 | 0.596302 |\n",
      "| simon                | 0.663507 | 0.624579 |\n",
      "| ngram                | 0.626777 | 0.601933 |\n",
      "| ensemble_lr          | 0.71564  | 0.689328 |\n",
      "| ensemble_rf          | 0.693128 | 0.675578 |\n",
      "| ensemble_lr+feats    | 0.709716 | 0.707557 |\n",
      "| ensemble_rf+feats    | 0.680095 | 0.664358 |\n",
      "| ensemble_lr+feats+mc | 0.7109   | 0.71138  |\n"
     ]
    }
   ],
   "source": [
    "print(metrics.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-lemon",
   "metadata": {},
   "source": [
    "# Export predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "athletic-domestic",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-379-1c39e7e9a0b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# i don't want to continue the execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "# i don't want to continue the execution\n",
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "played-while",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1656x34 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9936 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "considered-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model\n",
    "# lr + feats + meaningcloud\n",
    "X_dev_feats_mc =hstack((X_dev, feat1_dev.todense(), feat2_dev, meaningcloud_dev))\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear', max_iter=300, random_state=42,\n",
    "                        class_weight='balanced',)\n",
    "pipe = Pipeline([\n",
    "    ('pprocess', Normalizer()),\n",
    "    ('lr', lr),\n",
    "])\n",
    "distributions = dict(lr__C=uniform(loc=0, scale=10), lr__penalty=['l2', 'l1'],)\n",
    "clf = RandomizedSearchCV(pipe, distributions, random_state=42)\n",
    "clf.fit(X_dev_feats_mc, y_dev)\n",
    "\n",
    "\n",
    "X_test_feats_mc =hstack((X_test, feat1_test.todense(), feat2_test, meaningcloud_test))\n",
    "selected_test_preds = clf.best_estimator_.predict(X_test_feats_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "electronic-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(test['id'], selected_test_preds.astype(str)))\\\n",
    ".to_csv('preds_test/test_preds_ensemble_lr-feats-mc.tsv', header=None, index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
