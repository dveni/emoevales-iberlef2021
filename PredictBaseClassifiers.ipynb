{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expected-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rapid-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "sized-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from nlp import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "premier-station",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Using custom data configuration default\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', delimiter='\\t',\n",
    "                       data_files={'train': 'data/EmoEvalEs/train.tsv',\n",
    "                                    'validation': 'data/EmoEvalEs/dev.tsv',})\n",
    "test = load_dataset('csv', delimiter='\\t', data_files={'test': 'data/EmoEvalEs/emoevales_test.tsv'})\n",
    "_test = pd.read_csv('data/EmoEvalEs/emoevales_test.tsv', sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "beginning-representative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event</th>\n",
       "      <th>tweet</th>\n",
       "      <th>offensive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16b25dfb-e284-4a58-b62c-8186fc082eb6</td>\n",
       "      <td>GameOfThrones</td>\n",
       "      <td>Se√±or de luz, ven a nosotros en nuestra oscuri...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2a80f6bf-4750-4783-9bc8-fdb8ff2b94c4</td>\n",
       "      <td>SpainElection</td>\n",
       "      <td>Pues ya hemos votado tanto mi madre y yo #Elec...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1f477a6f-3559-41ee-8ec5-2e77aee53190</td>\n",
       "      <td>WorldBookDay</td>\n",
       "      <td>#DiaDelLibro üòá‚≠ê‚ù§Ô∏èüôèüôåüòçüå∑ sentimientos, viajar con...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>838add70-748c-4635-8133-36ff0b05aeb0</td>\n",
       "      <td>WorldBookDay</td>\n",
       "      <td>¬°Feliz #D√≠aDelLibroüìö! ‚ÄùEl libro es fuerza, es ...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64e3dd59-ae5c-4b9b-bd29-987609eb95d8</td>\n",
       "      <td>SpainElection</td>\n",
       "      <td>Pues qu√© quer√©is que os diga, este sarao pol√≠t...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id          event  \\\n",
       "0  16b25dfb-e284-4a58-b62c-8186fc082eb6  GameOfThrones   \n",
       "1  2a80f6bf-4750-4783-9bc8-fdb8ff2b94c4  SpainElection   \n",
       "2  1f477a6f-3559-41ee-8ec5-2e77aee53190   WorldBookDay   \n",
       "3  838add70-748c-4635-8133-36ff0b05aeb0   WorldBookDay   \n",
       "4  64e3dd59-ae5c-4b9b-bd29-987609eb95d8  SpainElection   \n",
       "\n",
       "                                               tweet offensive  \n",
       "0  Se√±or de luz, ven a nosotros en nuestra oscuri...        NO  \n",
       "1  Pues ya hemos votado tanto mi madre y yo #Elec...        NO  \n",
       "2  #DiaDelLibro üòá‚≠ê‚ù§Ô∏èüôèüôåüòçüå∑ sentimientos, viajar con...        NO  \n",
       "3  ¬°Feliz #D√≠aDelLibroüìö! ‚ÄùEl libro es fuerza, es ...        NO  \n",
       "4  Pues qu√© quer√©is que os diga, este sarao pol√≠t...        NO  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "important-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_preds(obj, name, fold):\n",
    "    path = 'preds_{}/{}.pck'.format(fold, name)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-brooklyn",
   "metadata": {},
   "source": [
    "# Base classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "disturbed-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "spoken-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsitk.preprocess import pprocess_twitter, Preprocessor\n",
    "\n",
    "text_train = Preprocessor(pprocess_twitter).transform(dataset['train']['tweet'])\n",
    "text_dev = Preprocessor(pprocess_twitter).transform(dataset['validation']['tweet'])\n",
    "text_test = Preprocessor(pprocess_twitter).transform(test['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "numerous-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = pd.Series(np.concatenate((text_train, text_dev, text_test), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "wired-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "ohe = OneHotEncoder()\n",
    "feat1_train = ohe.fit_transform(np.array(dataset['train']['event']).reshape(-1,1))\n",
    "feat1_dev = ohe.transform(np.array(dataset['validation']['event']).reshape(-1,1))\n",
    "feat1_test = ohe.transform(np.array(test['event']).reshape(-1,1))\n",
    "\n",
    "oe = OrdinalEncoder()\n",
    "feat2_train = oe.fit_transform(np.array(dataset['train']['offensive']).reshape(-1,1))\n",
    "feat2_dev = oe.transform(np.array(dataset['validation']['offensive']).reshape(-1,1))\n",
    "feat2_test = oe.transform(np.array(test['offensive']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-pierce",
   "metadata": {},
   "source": [
    "## SIMON custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "handmade-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "from gsitk.features import simon\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def simon_pipeline():\n",
    "    simon_pipe = Pipeline([\n",
    "        ('lr', LogisticRegressionCV(cv=10, random_state=42, n_jobs=-1, solver='liblinear'))\n",
    "    ])\n",
    "    return simon_pipe\n",
    "\n",
    "def generate_custom_lexicon(text):\n",
    "    filter_words = set(stopwords.words('spanish')) | set(string.punctuation)\n",
    "\n",
    "    counter = Counter(chain.from_iterable(text.str.split(' ').values))\n",
    "    selection = sorted([(word, count) for word, count in counter.items()], key=lambda wc: wc[1], reverse=True)\n",
    "    selection = [word for word, _ in selection if word not in filter_words]\n",
    "    selection = [selection]\n",
    "    return selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dental-edition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating custom lexicon\n",
      "Done\n",
      "Loading embeddings\n",
      "Done\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_resources' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a4a7fd9ff642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mspanish_lex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/SpanishSentimentLexicons/fullStrengthLexicon.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcustom_lexicon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membbeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m _simon_model = simon.Simon(lexicon=custom_lexicon,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_resources' is not defined"
     ]
    }
   ],
   "source": [
    "print('Generating custom lexicon')\n",
    "custom_lexicon = generate_custom_lexicon(all_texts)\n",
    "print('Done')\n",
    "\n",
    "# facebook fasttext embeddings\n",
    "print('Loading embeddings')\n",
    "embbeddings = KeyedVectors.load_word2vec_format(\n",
    "    '/home/jovyan/work/projects/data/WordEmbeddings/eng/crawl-300d-2M.vec', binary=False)\n",
    "print('Done')\n",
    "\n",
    "spanish_lex = pd.read_csv('data/SpanishSentimentLexicons/fullStrengthLexicon.txt', header=None, sep='\\t')[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "changing-trash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting SIMON feats\n",
      "Done\n",
      "Training classifier and predicting\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "_simon_model = simon.Simon(lexicon=custom_lexicon,\n",
    "                           n_lexicon_words=2000,\n",
    "                           embedding=embbeddings)\n",
    "simon_model_custom = simon.simon_pipeline(simon_transformer=_simon_model, percentile=50)\n",
    "\n",
    "print('Training and predicting SIMON feats')\n",
    "X_simon_train = simon_model_custom.fit_transform(pd.Series(text_train).str.split(' '), dataset['train']['emotion'])\n",
    "X_simon_dev = simon_model_custom.transform(pd.Series(text_dev).str.split(' '))\n",
    "X_simon_test = simon_model_custom.transform(pd.Series(text_test).str.split(' '))\n",
    "print('Done')\n",
    "\n",
    "# join with external feats\n",
    "X_simon_train = np.concatenate((X_simon_train, feat1_train.todense(), feat2_train), axis=1)\n",
    "X_simon_dev = np.concatenate((X_simon_dev, feat1_dev.todense(), feat2_dev), axis=1)\n",
    "X_simon_test = np.concatenate((X_simon_test, feat1_test.todense(), feat2_test), axis=1)\n",
    "\n",
    "\n",
    "print('Training classifier and predicting')\n",
    "simon_pipe = simon_pipeline()\n",
    "simon_pipe.fit(X_simon_train, dataset['train']['emotion'])\n",
    "simon_preds_dev = simon_pipe.predict(X_simon_dev)\n",
    "simon_preds_test = simon_pipe.predict(X_simon_test)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "accepted-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_preds(simon_preds_dev, 'simon', 'dev')\n",
    "save_preds(simon_preds_test, 'simon', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_simon_train\n",
    "del X_simon_dev\n",
    "del X_simon_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-asthma",
   "metadata": {},
   "source": [
    "## word2vecFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "photographic-understanding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting SIMON feats\n",
      "Done\n",
      "Training classifier and predicting\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from gsitk.features.word2vec import Word2VecFeatures\n",
    "\n",
    "w2v_transformer = Word2VecFeatures(model=embbeddings)\n",
    "\n",
    "print('Training and predicting SIMON feats')\n",
    "X_w2v_train = w2v_transformer.fit_transform(pd.Series(text_train).str.split(' '), dataset['train']['emotion'])\n",
    "X_w2v_dev = w2v_transformer.transform(pd.Series(text_dev).str.split(' '))\n",
    "X_w2v_test = w2v_transformer.transform(pd.Series(text_test).str.split(' '))\n",
    "print('Done')\n",
    "\n",
    "# join with external feats\n",
    "X_w2v_train = np.concatenate((X_w2v_train, feat1_train.todense(), feat2_train), axis=1)\n",
    "X_w2v_dev = np.concatenate((X_w2v_dev, feat1_dev.todense(), feat2_dev), axis=1)\n",
    "X_w2v_test = np.concatenate((X_w2v_test, feat1_test.todense(), feat2_test), axis=1)\n",
    "\n",
    "\n",
    "print('Training classifier and predicting')\n",
    "w2v_pipe = simon_pipeline()\n",
    "w2v_pipe.fit(X_w2v_train, dataset['train']['emotion'])\n",
    "w2v_preds_dev = w2v_pipe.predict(X_w2v_dev)\n",
    "w2v_preds_test = w2v_pipe.predict(X_w2v_test)\n",
    "print('Done')\n",
    "\n",
    "save_preds(w2v_preds_dev, 'w2v', 'dev')\n",
    "save_preds(w2v_preds_test, 'w2v', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-victoria",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "natural-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,2))),\n",
    "    ('lr', LogisticRegressionCV(cv=10, random_state=42, n_jobs=-1, solver='liblinear'))\n",
    "])\n",
    "tfidf_pipe.fit(text_train, dataset['train']['emotion'])\n",
    "tfidf_preds_dev = tfidf_pipe.predict(text_dev)\n",
    "tfidf_preds_test = tfidf_pipe.predict(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "african-fitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_preds(tfidf_preds_dev, 'tfidf', 'dev')\n",
    "save_preds(tfidf_preds_test, 'tfidf', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-research",
   "metadata": {},
   "source": [
    "## ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "intermediate-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_pipe = Pipeline([\n",
    "    ('ngram', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('lr', LogisticRegressionCV(cv=10, random_state=42, n_jobs=-1, solver='liblinear'))\n",
    "])\n",
    "ngram_pipe.fit(text_train, dataset['train']['emotion'])\n",
    "ngram_preds_dev = ngram_pipe.predict(text_dev)\n",
    "ngram_preds_test = ngram_pipe.predict(text_test)\n",
    "\n",
    "save_preds(ngram_preds_dev, 'ngram', 'dev')\n",
    "save_preds(ngram_preds_test, 'ngram', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-intelligence",
   "metadata": {},
   "source": [
    "## ngrams + feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "robust-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fresh-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "X_train = cv.fit_transform(text_train)\n",
    "X_dev = cv.transform(text_dev)\n",
    "X_test = cv.transform(text_test)\n",
    "\n",
    "X_train = hstack([X_train, feat1_train, feat2_train])\n",
    "X_dev = hstack([X_dev, feat1_dev, feat2_dev])\n",
    "X_test = hstack([X_test, feat1_test, feat2_test])\n",
    "\n",
    "\n",
    "ngram_pipe = Pipeline([\n",
    "    ('lr', LogisticRegressionCV(cv=10, random_state=42, n_jobs=-1, solver='liblinear'))\n",
    "])\n",
    "ngram_pipe.fit(X_train, dataset['train']['emotion'])\n",
    "ngram_preds_dev = ngram_pipe.predict(X_dev)\n",
    "ngram_preds_test = ngram_pipe.predict(X_test)\n",
    "\n",
    "save_preds(ngram_preds_dev, 'ngramfeats', 'dev')\n",
    "save_preds(ngram_preds_test, 'ngramfeats', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-swift",
   "metadata": {},
   "source": [
    "## meaningcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "interracial-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Desde Jupyterhub\n",
    "HOST = ''\n",
    "\n",
    "#ENDPOINT para sentimientos\n",
    "ENDPOINT = HOST + '/sentiment-2.1'\n",
    "\n",
    "with open('meaningcloud.key') as f:\n",
    "    KEY = f.read()\n",
    "    \n",
    "def analyze(txt, lang='en', model=None):\n",
    "    model = model or 'general_' + lang\n",
    "    res = requests.post(ENDPOINT, data={'key': KEY, 'txt': txt, 'lang': lang, 'model': model})\n",
    "    return res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "intermediate-facial",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-52450f7ae724>:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for instance in tqdm_notebook(dataset['train']['tweet']):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67564831090047e2a0494732d9560efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5723 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-52450f7ae724>:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for instance in tqdm_notebook(dataset['validation']['tweet']):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e2efcca8254b19956aad7c68160fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-52450f7ae724>:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for instance in tqdm_notebook(test['test']['tweet']):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d85bffe4c0427a80507d52d2b09a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1626 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meaningcloud_train, meaningcloud_dev, meaningcloud_test = [], [], []\n",
    "for instance in tqdm_notebook(dataset['train']['tweet']):\n",
    "    resp = analyze(instance, lang='es')\n",
    "    meaningcloud_train.append([resp['score_tag'], resp['agreement'], resp['subjectivity'],\n",
    "            resp['confidence'], resp['irony']])\n",
    "\n",
    "for instance in tqdm_notebook(dataset['validation']['tweet']):\n",
    "    resp = analyze(instance, lang='es')\n",
    "    meaningcloud_dev.append([resp['score_tag'], resp['agreement'], resp['subjectivity'],\n",
    "            resp['confidence'], resp['irony']])\n",
    "\n",
    "for instance in tqdm_notebook(test['test']['tweet']):\n",
    "    resp = analyze(instance, lang='es')\n",
    "    meaningcloud_test.append([resp['score_tag'], resp['agreement'], resp['subjectivity'],\n",
    "            resp['confidence'], resp['irony']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "adapted-serve",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-86-bb930fbbbec6>:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for instance in tqdm_notebook(test['tweet']):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6f33f6c6114861a9c599e927ea01db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meaningcloud_test = []\n",
    "for instance in tqdm_notebook(test['tweet']):\n",
    "    resp = analyze(instance, lang='es')\n",
    "    meaningcloud_test.append([resp['score_tag'], resp['agreement'], resp['subjectivity'],\n",
    "            resp['confidence'], resp['irony']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "vocational-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "oe = OrdinalEncoder()\n",
    "sent_train = ohe.fit_transform(pd.DataFrame(meaningcloud_train)[0].values.reshape(-1,1))\n",
    "others_train = oe.fit_transform(pd.DataFrame(meaningcloud_train)[[1,2,4]].values)\n",
    "confidence_train = pd.DataFrame(meaningcloud_train)[3].values.astype(int).reshape(-1,1)/100\n",
    "X_meaningcloud_train = np.concatenate((sent_train.todense(), others_train, confidence_train), axis=1)\n",
    "\n",
    "sent_dev = ohe.transform(pd.DataFrame(meaningcloud_dev)[0].values.reshape(-1,1))\n",
    "others_dev = oe.fit_transform(pd.DataFrame(meaningcloud_dev)[[1,2,4]].values)\n",
    "confidence_dev = pd.DataFrame(meaningcloud_dev)[3].values.astype(int).reshape(-1,1)/100\n",
    "X_meaningcloud_dev = np.concatenate((sent_dev.todense(), others_dev, confidence_dev), axis=1)\n",
    "\n",
    "\n",
    "sent_test = ohe.transform(pd.DataFrame(meaningcloud_test)[0].values.reshape(-1,1))\n",
    "others_test = oe.fit_transform(pd.DataFrame(meaningcloud_test)[[1,2,4]].values)\n",
    "confidence_test = pd.DataFrame(meaningcloud_test)[3].values.astype(int).reshape(-1,1)/100\n",
    "X_meaningcloud_test = np.concatenate((sent_test.todense(), others_test, confidence_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "suspended-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feats(obj, name, fold):\n",
    "    path = 'feats/{}_{}.pck'.format(name, fold)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "        \n",
    "save_feats(X_meaningcloud_train, 'meaningcloud', 'train')\n",
    "save_feats(X_meaningcloud_dev, 'meaningcloud', 'dev')\n",
    "save_feats(X_meaningcloud_test, 'meaningcloud', 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
